m4_include(/mcs/m4/worksp.lib.m4)
_NIMBUS_HEADER(2.4 Admin Reference)
_NIMBUS_HEADER2(n,n,y,n,n,n,n)
_NIMBUS_LEFT2_COLUMN
_NIMBUS_LEFT2_ADMIN_SIDEBAR(n,n,y,n,n)
_NIMBUS_LEFT2_COLUMN_END
_NIMBUS_CENTER2_COLUMN
_NIMBUS_2_4_DEPRECATED

<h2>Nimbus 2.4 Admin Reference</h2>

<p>
    This section explains some side tasks as well as some
    none-default configurations.
</p>

<ul>
    <li>
        <p>
            <a href="#confs">Notes on conf files</a>
        </p>
    </li>
    <li>
        <p>
            <a href="#elastic">Enabling the EC2 SOAP frontend</a>
        </p>
    </li>
    <li>
        <p>
            <a href="#query">Configuring the EC2 Query frontend</a>
        </p>
    </li>
	<li>
        <p>
            <a href="#nimbusweb-config">Configuring the Nimbus Web interface</a>
        </p>
    </li>
    <li>
        <p>
            <a href="#nimbusweb-usage">Using the Nimbus Web interface</a>
        </p>
    </li>
    <li>
        <p>
            <a href="#cert-pointing">Configuring a different host certificate</a>
        </p>
    </li>
    <li>
        <p>
            <a href="#manual-nimbus-basics">Configuring Nimbus basics manually without the auto-configuration program</a>
        </p>
    </li>
    <li>
        <p>
            <a href="#resource-pool-and-pilot">Resource pool and pilot configurations</a>
        </p>
        <ul>
            <li>
                <a href="#resource-pool">Resource pool</a>
            </li>
            <li>
                <a href="#pilot">Pilot</a>
            </li>
        </ul>
    </li>
    <li>
        <p>
            <a href="#backend-config-invm-networking">Network configuration
                details</a>
        </p>
    </li>
    <li>
        <p>
            <a href="#context-broker-standalone">Configuring a standalone context broker</a>
        </p>
    </li>

</ul>

<!-- *********************************************************************** -->
<!-- *********************************************************************** -->
<!-- *********************************************************************** -->
<br />

<a name="confs"> </a>
<h2>Notes on conf files _NAMELINK(confs)</h2>

<p>
    The Nimbus conf files have many comments around each configuration.  Check
    those out.  Their content will be inlined here in the future.
</p>

<p>
    See the <i>$NIMBUS_HOME/services/etc/nimbus/workspace-service</i> directory.
</p>

<!-- *********************************************************************** -->
<!-- *********************************************************************** -->
<!-- *********************************************************************** -->


<br />

<a name="elastic"> </a>
<h2>Enabling the EC2 SOAP frontend _NAMELINK(elastic)</h2>
<p>
    After installing, see the <i>$NIMBUS_HOME/services/etc/nimbus/elastic</i>
    directory.  The .conf file here specifies what the EC2 "instance type"
    allocations should translate to and what networks should be requested
    from the underlying workspace service when VM create requests are sent.
</p>

<p>
    By default, a Nimbus installation will enable this service:
</p>

<div class="screen"><pre>
https://10.20.0.1:8443/wsrf/services/ElasticNimbusService
</pre></div>

<p>
    But before the service will work, you must adjust a container configuration.
    This will account for some security related customs for EC2: 
</p>
<ul>
    <li>
        <p>
            Secure message is used, but only on the request.  No secure message
            envelope is sent around EC2 responses, therefore EC2 clients do not
            expect such a response.  It relies on the fact that <b>https</b>
            is being used to protect responses.
        </p>
        <p>
            Both integrity and encryption problems are relevant, be wary of any
            http endpoint being used with this protocol.  For example, you
            probably want to make sure that add-keypair private key
            responses are encrypted (!).
        </p>
    </li>
    <li>
        <p>
            Also, adjusting the container configuration gets around a timestamp
            format incompatibility we discovered (the timestamp is normalized
            <i>after</i> the message envelope signature/integrity is confirmed).
        </p>
    </li>
</ul>

<p>
    There is a sample container <i>server-config.wsdd</i> configuration
    to compare against
    <a href="sample-server-config.wsdd-supporting-ec2.xml">here</a>.
</p>

<p>
    Edit the container deployment configuration:
</p>
_EXAMPLE_CMD_BEGIN
nano -w etc/globus_wsrf_core/server-config.wsdd
_EXAMPLE_CMD_END

<p>
    Find the <b>&lt;requestFlow&gt;</b> section and comment out the existing
    <i>WSSecurityHandler</i> and add this new one:
</p>

<div class="screen"><pre>
    &lt;handler type="java:org.globus.wsrf.handlers.JAXRPCHandler"&gt;

        &lt;!-- <b>enabled</b>: --&gt;
        &lt;parameter name="className"
                   value="org.nimbustools.messaging.gt4_0_elastic.rpc.WSSecurityHandler" /&gt;

        &lt;!-- <b>disabled</b>: --&gt;
        &lt;!--&lt;parameter name="className"
                   value="org.globus.wsrf.impl.security.authentication.wssec.WSSecurityHandler"/&gt; --&gt;
    &lt;/handler&gt;
</pre></div>


<p>
    Now find the <b>&lt;responseFlow&gt;</b> section and comment out the existing
    <i>X509SignHandler</i> and add this new one:
</p>

<div class="screen"><pre>
    &lt;handler type="java:org.apache.axis.handlers.JAXRPCHandler"&gt;

        &lt;!-- <b>enabled</b>: --&gt;
        &lt;parameter name="className"
                   value="org.nimbustools.messaging.gt4_0_elastic.rpc.SignHandler" /&gt;

        &lt;!-- <b>disabled</b>: --&gt;
        &lt;!--&lt;parameter name="className" 
                       value="org.globus.wsrf.impl.security.authentication.securemsg.X509SignHandler"/&gt;--&gt;
    &lt;/handler&gt;</pre></div>

<p>
    If you don't make this configuration, you will see
    <a href="troubleshooting.html#elastic-timestamp-error">this error</a>
    when trying to use an EC2 client.
</p>
<p>
    Container restart required after the configuration change.
</p>

<!-- *********************************************************************** -->
<!-- *********************************************************************** -->
<!-- *********************************************************************** -->

<br />

<a name="query"> </a>
<h2>Configuring the EC2 Query frontend _NAMELINK(query)</h2>
<p>
The EC2 Query frontend supports the same operations as the SOAP frontend.
However, it does not run in the same container. It listens on HTTPS using Jetty.
Starting with Nimbus 2.4, the query frontend is enabled and listens on port 8444.
</p>

<p>
Configuration for the query frontend lives in the
    <i>$NIMBUS_HOME/services/etc/nimbus/query</i> directory.
It contains a configuration file <i>query.conf</i> and a sample user mapping
file <i>users.txt</i>.
</p>

<p>
The Query interface does not rely on X509 certificates for security. Instead,
it uses a symmetric signature-based approach. Each user is assigned an access
identifier and secret key. These credentials are also maintained by the service.
Each request is "signed" by the client by generating a hash of parts of the
request and attaching them. The service performs this same signature process
and compares its result with the one included in the request.
</p>

<p>
For the initial release of the Query frontend, Nimbus users are mapped to
Query credentials in a flat text file. By default, this file is located at
<i>$GLOBUS_LOCATION/etc/nimbus/query/users.txt</i> but it may be placed
elsewhere by altering the <i>query.usermap.path</i> configuration value in
<i>query.conf</i>. This file must be manually managed but in the near future
it will be tied into the administrative web application. Changes to this file
do not require a container restart.
</p>

<p>
    To ease the process of authorizing users for the Query interface during the
    period before support is added to the administrative web application, we have
    added a utility to the <i>cloud-admin.sh</i> tool. This utility generates
    a secret key for a user and adds an entry to the <i>users.txt</i> file.
</p>

<div class="screen"><pre>
# <b>./cloud-admin.sh --add-query-dn "/O=Grid/OU=GlobusTest/OU=uchicago.edu/CN=Some User"</b>
Generated query credentials for user:
	Access ID: b9747c9a
	Secret key: vay/1xelRSr9Koq2MX09S+SvD3vrSQIsmfO4Cq16fZY=
*Securely* distribute these tokens to the user.

</pre></div>

<p>
    This utility is by no means a complete administrative tool for query users.
    For all other management, you'll need to edit the <i>users.txt</i> file
    directly.
</p>

<p>
    In addition to this utility, there is basic support for distributing query tokens
    via the <a href="../faq.html#nimbusweb">Nimbus Web</a> application. The admin
    can paste a user's credentials into the app and the user can retrieve them
    with their username and password. In the near future this functionality will be
    greatly expanded to allow management of tokens directly in this interface.
</p>

<!-- *********************************************************************** -->
<!-- *********************************************************************** -->
<!-- *********************************************************************** -->

<br />

<a name="nimbusweb-config"> </a>
<h2>Configuring the Nimbus Web interface <span class="namelink"><a href="#nimbusweb-config">(#)</a></span>
</h2>

<p>
    Starting with Nimbus 2.4, the Nimbus Web application is bundled with the service
    distribution but is disabled by default. To enable it, edit the
    <i>$NIMBUS_HOME/nimbus-setup.conf</i>
    file and change the value of <i>web.enabled</i> to <b>True</b>. Next you should run
    <i>nimbus-configure</i> to propagate the change. Now you can use <i>nimbusctl</i>
    to start/stop the web application.
</p>
<p>
    By default, the web application listens on port 1443. This and other configuration
    options are location in the <i>$NIMBUS_HOME/web/nimbusweb.conf</i> file. Changes to
    this file require a restart of the service.
</p>

<!-- *********************************************************************** -->
<!-- *********************************************************************** -->
<!-- *********************************************************************** -->

<br />

<a name="nimbusweb-usage"> </a>
<h2>Using the Nimbus Web interface <span class="namelink"><a href="#nimbusweb-usage">(#)</a></span>
</h2>

<p>
    While the Nimbus Web application is still under heavy development, its current
    functionality is useful enough to release in its early form. In the 2.4 release,
    Nimbus Web provides basic facilities for distributing new X509 credentials and EC2 query tokens to users.
    Previously this was a tedious process that was difficult to do in a secure way
    that was also user friendly. Nimbus Web allows an admin to upload credentials
    for a user and then send them a custom URL which invites them to create an
    account.
</p>
<p>
    To get started, log into the web interface as a superuser and go to the
    <i>Administrative Panel</i>. This page has a section for creating users
    as well as viewing pending and existing users. The initial release does not
    have embedded CA functionality (it is being planned). You must use an
    external CA to generate the user credentials and there is no way to 
    distribute a password so the key should be passwordless. You must also 
    manually authorize the new user for the
    Nimbus service (it is recommended that you use the cloud-admin tool for 
    quickly adding new users).
</p>

<p>
    Create a new user by filling in the appropriate fields and uploading an X509
    certificate and key for the user. Note that the application expects plain
    text files, so depending on your browser you may need to rename files to have
    a .txt extension before you can upload them. Once the new account is created,
    you will be provided with a custom URL. You must paste this URL into an email
    to the user along with usage instructions.
</p>

<p>
    When the user accesses the custom URL, they will be asked to create a password
    and login. Inside their account, they can download the certificate and key which
    were provided for them by the admin. Note that the design of the application attempts
    to maximize the security of the process, with several important features:
</p>
<ul>
    <li>
        <b>The URL token can only be successfully used once</b>. After a
        user creates a password and logs in, future attempts to access that URL will
        fail. This is to prevent someone from intercepting the URL and using it to
        access the user's credentials. If this happens, the real user will be unable
        to login and will (hopefully) contact the administrator immediately (there is a message urging them to do so).
    </li>
    <li>
        In the same spirit, <b>the URL token will expire</b> after a configurable number of
        hours (default: 12).
    </li>
    <li>
        <b>The user private key can be downloaded once and only once</b>.
        After this download occurs,
        the key will be deleted from the server altogether. In an ideal security system,
        no person or system will ever be in possession of a private key, except for the
        user/owner of the key itself. Because we don't follow this for the sake of usability,
        we attempt to minimize the time that the private key is in the web app database.
    </li>
    <li>
        When a URL token is accessed or a private key is downloaded, the time and IP address of
        this access is logged and displayed in the administrative panel. 
    </li>
</ul>
      

<!-- *********************************************************************** -->
<!-- *********************************************************************** -->
<!-- *********************************************************************** -->
<br />

<a name="cert-pointing"> </a>
<h2>Configuring a different host certificate _NAMELINK(cert-pointing)</h2>

<p>
    The Nimbus installer creates a Certificate Authority which is used for
    (among other things) generating a host certificate for the various services.
    There are three files involved in your host certificate and they
    are all generated during the install by the <em>nimbus-configure</em> program.
    By default, these files are placed in "$NIMBUS_HOME/var/" but you can control
    their placement with properties in the "$NIMBUS_HOME/nimbus-setup.conf" file.
</p>
<ul>
    <li><b>hostcert.pem</b> - The host certificate.
        The certificate for the issuing CA must be in the Nimbus trusted-certs
        directory, in hashed format.
    </li>
    <li><b>hostkey.pem</b> - The private key. Must be unencrypted and readable
    by the Nimbus user.</li>
    <li><b>keystore.jks</b> - Some Nimbus services require this special Java
    Key Store format. The <em>nimbus-configure</em> program generates this
    file from the host cert and key. If you delete the file, it can
    be regenerated by running <em>nimbus-configure</em> again.</li>
</ul>
<p>
    To use a custom host certificate, you can delete (or relocate)
    these three files, copy in your own hostcert.pem and hostkey.pem, and run
    <em>nimbus-configure</em>, which will generate the keystore.
</p>
<p>
    <b>NOTE:</b>
    It is important that the issuing CA cert is trusted by Nimbus (and any
    clients used to access the Nimbus services). This is done by placing the
    hashed form of the CA files in the trusted-certs directory, by default
    "$NIMBUS_HOME/var/ca/trusted-certs/". For example, these three files:
</p>
<div class="screen"><pre>
3fc18087.0
3fc18087.r0
3fc18087.signing_policy
</pre></div>
<p>
    If you simply want to generate new host certificates using the Nimbus
    internal CA (perhaps using a different hostname), you can follow a
    similar procedure. Delete or relocate the hostcert.pem, hostkey.pem,
    and keystore.jks files and then run <em>nimbus-configure</em>. New
    files will be generated.
</p>

<p>
    You can also keep these files outside of the Nimbus install (for example if
    you use the same host certificate for multiple services on the same machine.
    Just edit the $NIMBUS_HOME/nimbus-setup.conf file and adjust the hostcert,
    hostkey, and keystore properties. Then run <em>nimbus-configure</em>. If these files
    do not exist, they will be created.
</p>

<!-- *********************************************************************** -->
<!-- *********************************************************************** -->
<!-- *********************************************************************** -->

<br />

<a name="manual-nimbus-basics"> </a>
<h2>Configuring Nimbus basics manually without the auto-configuration program _NAMELINK(manual-nimbus-basics)</h2>

<p>
    What follows is the instructions for setting up a container as they existed
    before the auto-configuration program or the installer came into being (see
    <a href="quickstart.html#part-IIb">here</a> for
    information about the auto-configuration program).
</p>

<hr />


<h4>* Service hostname:</h4>
<p>
    Navigate to the workspace-service configuration directory:
</p>

_EXAMPLE_CMD_BEGIN
cd $GLOBUS_LOCATION/etc/nimbus/workspace-service
_EXAMPLE_CMD_END


<p>
    Edit the "ssh.conf" file:
</p>

_EXAMPLE_CMD_BEGIN
nano -w ssh.conf
_EXAMPLE_CMD_END


<p>
    Find this setting:
</p>

<div class="screen"><pre>
service.sshd.contact.string=REPLACE_WITH_SERVICE_NODE_HOSTNAME:22</pre>
</div>

<p>
    ... and replace the CAPS part with your service node hostname.  This
    hostname and port should be accessible from the VMM nodes.
</p>

<p>
    (The guide assumes you will have the same privileged account name on the
     service node and VMM nodes, but if not, this is where you would make the
     changes as you can read in the ssh.conf file comments).
</p>


<h4>* VMM names:</h4>
<p>
    Navigate to the workspace service VMM pools directory:
</p>

_EXAMPLE_CMD_BEGIN
cd $GLOBUS_LOCATION/etc/nimbus/workspace-service/vmm-pools
_EXAMPLE_CMD_END

<p>
    Each file in this directory represents a distinct pool of VMM nodes that
    are available for the service to run VMs on.  Leave this as one pool (one
    file, the example file).
</p>

<p>
    Edit the example "pool1" file to list only one test VMM node, the node
    where you installed Xen above.  List the amount of memory you would like
    allocated to the guest VMs.
</p>


<div class="screen"><pre>
some-vmm-node 1024</pre>
</div>

<p>
    You can SSH there without password from the nimbus account, right?
</p>

_EXAMPLE_CMD_BEGIN
ssh some-vmm-node
_EXAMPLE_CMD_END

<div class="screen"><pre>
nimbus@some-vmm-node $ ...</pre>
</div>

<a name="networks"> </a>
<h4>* Networks:</h4>
<p>
    Navigate to the workspace service networks directory:
</p>

_EXAMPLE_CMD_BEGIN
cd $GLOBUS_LOCATION/etc/nimbus/workspace-service/network-pools/
_EXAMPLE_CMD_END

<p>
    The service is packaged with two sample network files, <i>public</i> and
    <i>private</i>.
</p>
<p>
    You can name these files anything you want.  The file names will be the
    names of the networks that are offered to clients.  It's a convention to
    provide "public" and "private" but these can be anything.
</p>

<p>
    The <i>public</i> file has some comments in it.  Edit this file to have
    the one DNS line at the top and one network address to give out.  The
    subnet and network you choose should be something the VMM node can bridge
    to (there are some advanced configs to be able to do DHCP and bridge
    addresses for addresses that are foreign to the VMM, but this is not
    something addressed in this guide).
</p>

_EXAMPLE_CMD_BEGIN
nano -w public
_EXAMPLE_CMD_END

<div class="screen"><pre>
192.168.0.1
fakepub1 192.168.0.3 192.168.0.1 192.168.0.255 255.255.255.0</pre>
</div>




<!-- *********************************************************************** -->
<!-- *********************************************************************** -->
<!-- *********************************************************************** -->

<br />


<a name="resource-pool-and-pilot"> </a>
<h2>Resource pool and pilot configurations _NAMELINK(resource-pool-and-pilot)</h2>
<p>
    There are modules for two resource management strategies currently
    distributed with Nimbus: the default "resource pool" mode and the "pilot"
    mode.
</p>
<p>
    The "resource pool" mode is where the service has direct control of a pool
    of VMM nodes.  The service assumes it can start VMs 
</p>
<p>
    The "pilot" mode is where the service makes a request to a cluster's
    Local Resource Management System (LRMS) such as PBS.  The VMMs are equipped
    to run regular jobs in domain 0.  But if pilot jobs are submitted, the nodes
    are secured for VM management for a certain time period by the workspace
    service.  If the LRM or
    administrator preempts/kills the pilot job earlier than expected, the VMM
    is no longer available to the workspace service. 
</p>
<p>
    The "etc/nimbus/workspace-service/other/<b>resource-locator-ACTIVE.xml</b>"
    file dictates what mode is in use (container restart required if this
    changes).  See the available
    "etc/nimbus/workspace-service/other/<b>resource-locator-*</b>" files.
</p>

<a name="resource-pool"> </a>
<h3>Resource pool _NAMELINK(resource-pool)</h3>
<p>
    This is the default, see the <a href="#resource-pool-and-pilot">overview</a>.
</p>

<p>
    A directory of files is located at
    "etc/nimbus/workspace-service/<b>vmm-pools</b>/"
</p>
<p>
   The pool file format is currently very simple: for each node in the pool,
   list the hostname and the amount of RAM it can spare for running guest VMs.
</p>
<p>
   Optionally, you can also specify that certain hosts can only
   support a subset of the available networking associations (see the
   file comments for syntax).
</p>
<p>
   If you change these configuration files after starting the
   container, only a fresh container reboot will actually incur the
   changes.
</p>

<ul>

    <li>
        If you <b>add</b> a node, this will be available
        immediately after the container reboot.
    </li>

    <li>
        If you <b>remove</b> a node that is currently
        in use, no new deployments will be mapped to this VMM.
        However, this will not destroy (or migrate) any current VMs
        running there.  If that is necessary it currently needs to be
        accomplished explicitly.
    </li>

    <li>
        If you <b>change</b> a node that is currently
        in use, the change will take effect for the next lease.

        <p>
        If you've removed support for an association on a VMM that
        the current VM(s) is using, this change will not destroy (or
        migrate) the VM(s) to adjust to this restriction.
        If that is necessary it currently needs to be accomplished
        explicitly.
        </p>


        <p>
        If you've reduced the memory allocation below what the
        current VM(s) on the node is/are currently using, this will
        not destroy (or migrate) the current VM(s) to adjust to this
        restriction.
        If that is necessary it currently needs to be accomplished
        explicitly.  Once the VM(s) are gone, the maximum memory
        available on that VMM will be the new, lower maximum.
        </p>
    </li>

</ul>


<a name="pilot"> </a>
<h3>Pilot _NAMELINK(pilot)</h3>
<p>
    
</p>



<ol>
    <li>
        <p>
            The first step to switching to the pilot based infrastructure
            is to make sure you have at least one working node configured
            with workspace-control, following the instructions in this
            guide as if you were not going to use dynamically allocated
            VMMs via the pilot.
        </p>

        <p>
            If the only nodes available are in the LRM pool, it would be best
            to drain the jobs from one and take it offline while you confirm
            the setup.
        </p>
    </li>

    <li>
        <p>
            Next, make sure that the system account the container is
            running in can submit jobs to the LRM.  For example, run
            <b>echo "/bin/true" | qsub</b>
        </p>
    </li>

    <li>
        <p>
            Next, decide how you would like to organize the cluster nodes,
            such that the request for time on the nodes from the workspace
            service in fact makes it end up with usable VMM nodes.
        </p>
        <p>
            For example, if there are only a portion of nodes configured
            with Xen and workspace-control, you can set up a special node
            property (e.g. 'xen') or perhaps a septe queue or server.
            The service supports submitting jobs with node property
            requirements and also supports the full Torque/PBS
            '[queue][@server]' destination syntax if desired.
        </p>
    </li>

    <li>
        <p>
            Copy the
            "etc/nimbus/workspace-service/other/<b>resource-locator-pilot.xml</b>"
            to
            "etc/nimbus/workspace-service/other/<b>resource-locator-ACTIVE.xml</b>"
        </p>

        <p>
            The configuration comments in "etc/nimbus/workspace-service/<b>pilot.conf</b>"
            should be self explanatory.  There are a few to highlight here
            (and note that advanced configs are in <b>resource-locator-ACTIVE.xml</b>).
        </p>

        <ul>
            <li>
                <p>
                    HTTP digest access authentication based notifications
                    is a mechanism for pilot notifications.  Each message
                    from a pilot process to the workspace service takes
                    on the order of 10ms on our current testbed which is
                    reasonable.
                </p>

                <p>
                    The <b>contactPort</b> setting is used
                    to control what port the embedded HTTP server listens
                    on.  It is also the contact URL passed to the pilot
                    program, an easy way to get this right is to use an
                    IP address rather than a hostname.
                </p>

                <p>
                    Note the <b>accountsPath</b> setting.
                    Navigate to that file ("etc/nimbus/workspace_service/<b>pilot-authz.conf</b>"
                    by default) and change the shared secret to something
                    not dictionary based and 15 or more characters.
                    A script in that directory will produce suggestions.
                </p>

                <p>
                    This port may be blocked off entirely from WAN access
                    via firewall if desired, only the pilot programs need
                    to connect to it.  If it is not blocked off, the use
                    of HTTP digest access authentication for connections
                    is still guarding access.
                </p>

                <p>
                    Alternatively, you can configure only SSH for these
                    notifications as well as configure both and use SSH as
                    a fallback mechanism.
                    When used as a fallback mechanism, the pilot will try to
                    contact the HTTP server and if that fails will then
                    attempt to use SSH.  Those message are written to a
                    file and will be read when the workspace service
                    recovers.  This is an advanced configuration, setting
                    up the infrastructure without this configured is recommended
                    for the first pass (reduce your misconfiguration chances).
                </p>

            </li>

            <li>
                <p>
                    The <b>maxMB</b> setting is used to
                    set a hard maximum memory allotment across all workspace
                    requests (no matter what the authorization layers
                    allow).  This a "fail fast" setting, making sure
                    dubious requests are not sent to the LRM.
                </p>
                <p>
                    To arrive at that number, you must arrive at the
                    maximum amount of memory
                    to give domain 0 in non-hosting mode.  This should be
                    as much as possible and you will also configure this later
                    into the pilot program settings (the pilot will make sure domain
                    0 gets this memory back when returning the node from hosting
                    mode to normal job mode).
                </p>
                <p>
                    When the node boots and xend is first run, you should
                    configure things such that domain 0 is already at this
                    memory setting.  This way, it will be ready to give
                    jobs as many resources as possible from its initial
                    boot state.
                </p>
                <p>
                    Domain 0's memory is set in the boot pmeters.
                    On the "kernel" line you can add a parameter like this:
                    <b>dom0_mem=2007M</b>
                </p>
                <p>
                    If it is too high you will make the node unbootable,
                    2007M is an example from a 2048M node and was arrived
                    at experimentally.  We are working on ways to
                    automatically figure out the highest number
                    this can be without causing boot issues.
                </p>
                <p>
                    Take this setting and subtract at least 128M from it,
                    allocating the rest for guest workspaces.  Let's label
                    128M in this example as <b>dom0-min</b>
                    and 2007 as <b>dom0-max</b>.  Some memory
                    is necessary for domain 0 to at least do privileged
                    disk and net I/O for guest domains.
                </p>
                <p>
                    These two memory setting will be configured into the
                    pilot to make sure domain 0 is always in the correct
                    state.  Domain 0's memory will never be set below the
                    <b>dom0-min</b> setting and will always
                    be returned to the <b>dom0-max</b> when
                    the pilot program vacates the node.
                </p>
                <p>
                    Instead of letting the workspace request fail on the
                    backend just before instantiation, the
                    <b>maxMB</b> setting is configured in
                    the service so b requests for more memory will be
                    rejected up front.
                </p>
                <p>
                    So [ <b>dom0-max</b> minus
                    <b>dom0-min</b> equals
                    <b>maxMB</b> ].  And again
                    <b>maxMB</b> is the maximum allowed for
                    guest workspaces.
                </p>
                <p>
                    ( You could make it smaller.  But it would not make
                    sense to make it bigger than
                    [ <b>dom0-max</b> minus
                    <b>dom0-min</b> ] because this will
                    cause the pilot program itself to reject the request. )
                </p>
            </li>

            <li>
                <p>
                    The <b>pilotPath</b> setting must be
                    gotten right and double checked.
                    See
                    <a href="http://bugzilla.globus.org/bugzilla/show_bug.cgi?id=5869">this
                    bugzilla item</a>
                </p>
            </li>

        </ul>

    </li>

    <li>
        <p>
            Next, note your pilotPath setting and put a copy of
            <b>workspacepilot.py</b> there. Run
            <b>chmod +x</b> on it and that is all
            that should be necessary for the installation.
        </p>
        <p>
            Python 2.3 or higher (though not Python 3.x) is also required
            but this was required for workspace-control as well.
        </p>
        <p>
            A sudo rule to the xm program is also required but this was
            configured when you set up workspace-control.  If the account
            the pilot jobs are run under is different than the account that
            runs workspace-control, copy the xm sudo rule for the account.
        </p>
    </li>

    <li>
        <p>
            Open the <b>workspacepilot.py</b> file in an
            editor.  These things must be configured correctly and require
            your intervention (i.e., the software cannot guess at them):
        </p>

        <ul>

            <li>
                Search for "<b>secret: pw_here</b>" around
                line 80.  Replace "pw_here" with the shared secret you
                configured above.
            </li>

            <li>
                Below that, set the "minmem" setting to the value you chose
                above that we called <b>dom0-min</b>.
            </li>

            <li>
                Set the "dom0_mem" setting to the value you chose above
                that we called <b>dom0-max</b>.
            </li>

        </ul>

        <p>
            The other configurations should be explained enough in the
            comments and they also usually do not need to be altered.
        </p>
        <p>
            You might like to create a directory for the pilot's logfiles
            instead
            of the default setting of "/tmp" for the "logfiledir"
            configuration.  You might also wish to septe out the config
            file from the program.  The easiest way to do that is
            to configure the service to call a shell script instead of
            workspacepiloy.py.  This in turn could wrap the call to the
            pilot program, for example:
            <b>"/opt/workspacepilot.py -p /etc/workspace-pilot.conf $@"</b>
        </p>

    </li>

    <li>
        Now restart the GT container and submit test workspace requests as
        usual (cloud requests work too).
    </li>

</ol>









<!-- *********************************************************************** -->
<!-- *********************************************************************** -->
<!-- *********************************************************************** -->



<br />


<a name="backend-config-invm-networking"> </a>
<h2>Network configuration details _NAMELINK(backend-config-invm-networking)</h2>
<p>
    For the Workspace backend to support networking information delivery to
    VMs, you are required to install DHCP and ebtables on each hypervisor
    node. When networking information in a workspace needs to change at
    its startup (which is typical upon deployment), workspace-control will
    make a call via sudo to a program that adds a MAC address to IP mapping
    into the local DHCP server for each of the workspace's NICs that need
    to be configured. It will also adjust ebtables rules for each of the
    workspace's NICs that need to be configured: these make sure the NICs
    are using the proper MAC and IP address as well as directing DHCP
    requests to the local DHCP server only.
</p>

<p>
    To actually enact networking changes, the VM must set its own L3 networking
    information (IP address, default gateway, etc) from inside the VM. Currently
    we only support delivery of the information via DHCP. Booting into DHCP
    client mode is well supported in virtually every operating system in
    existence. Previously we passed information via kernel parameters which
    required a special understanding inside the VM. The result of using DHCP
    is that workspace images are easier to create and easier to maintain.
</p>

<p>
    A DHCP server is required to run on each hypervisor node. The purpose of
    this server is to respond to broadcast requests from workspace's that
    are booting locally. Before starting a VM, if any of its NICs need to
    be configured via DHCP, workspace-control will call out to "dhcp-config.sh"
    via sudo, passing it a specific MAC address to IP mapping (as well as
    other information to be passed to the VM such as hostname, dns servers,
    broadcast address, default gateway, etc).
</p>

<ul>
    <li>"Won't this interfere with my current DHCP server?" No.</li>
    <li>"Will this cause unwanted packets on my physical LAN?" No.</li>
    <li>"Will other workspaces be able to send broadcasts and get the wrong DHCP lease?" No.</li>
</ul>

<p>
    In addition to a DHCP server, we also insert custom ebtables rules when the
    workspace is deployed. These rules accomplish three things:
</p>

<ol>
    <li>Only packets with the correct MAC address for this virtual interface
        are permitted.</li>
    <li>Broadcasted DHCP requests are only permitted to be bridged to the
        correct virtual interface (configured in workspace-control's
        configuration file).</li>
    <li>Only packets with the correct IP address for this virtual
        interface are permitted (as the NIC does not have an IP address yet
        when making a DHCP request, the IP check only happens if it is not
        a DHCP request.</li>
</ol>
<p>
    A version of the workspace DHCP design document is available here:
    <a href="../DHCP-TP1.2.2-draft2.pdf">pdf</a>.
</p>

<!-- *********************************************************************** -->
<!-- *********************************************************************** -->
<!-- *********************************************************************** -->

<br />


<a name="context-broker-standalone"> </a>
<h2>Configuring a standalone context broker _NAMELINK(context-broker)</h2>

<p>
    The <a href="../faq.html#ctxbroker">context broker</a> is used to
    facilitate <a href="../clouds/clusters.html">one click clusters</a>.
</p>
<p>
    The context broker (see <a href="#context-broker">above</a>)
    is installed and configured automatically starting with Nimbus 2.4, but there
    is not a dependency on any Nimbus service component.  It can run by itself in a
    GT container.  You can use it for deploying virtual clusters on EC2 for
    example without any other Nimbus service running (the cloud client #11 has
    an "ec2script" option that will allow you to do this).
</p>
<p>
    If you want to install the broker separately from Nimbus, download the 
    Nimbus source tarball, extract it, and run 
    <i>scripts/broker-build-and-install.sh</i> with an appropriate 
    <b>$GLOBUS_LOCATION</b> set.
</p>
<p>
    To set up a standalone broker that is compatible with post-010 cloud clients,
    follow these steps:
</p>
<ol>
    <li>
        <p>
            Create a passwordless CA certificate.
        </p>

        <p>
            You can do this from an existing CA.  To unencrypt an RSA key, run:
            <i>openssl rsa -in cakey.pem -out cakey-unencrypted.pem</i>
        </p>
        <p>
            Alternatively, you can use the CA created by the Nimbus installer
            under $NIMBUS_HOME/var/ca
        </p>
    </li>
    <li>
        <p>
            Make very sure that the CA certificate and key files are read-only
            and private to the container running account.
        </p>
    </li>
    <li>
        <p>
            Add the CA certificate to your container's trusted certificates
            directory.  The context broker (running in the container) creates
            short term credentials on the fly for the VMs.  The VMs use this
            to contact the broker: the container needs to be able to verify
            who is calling.
        </p>
    </li>
    <li>
        <p>
           Navigate to "<i>$GLOBUS_LOCATION/etc/nimbus-context-broker</i>"
           and adjust the "caCertPath" and "caKeyPath" parameters in the
           "<i>jndi-config.xml</i>" file to point to the CA certificate
            and key files you created in previous steps.
        </p>
        <p>
            Note that the old and new context brokers can both use the same
            CA certificate and key file.
        </p>
    </li>
    <li>
        <p>
            Container restart is required.
        </p>
    </li>
</ol>
        

<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />

_NIMBUS_CENTER2_COLUMN_END
_NIMBUS_FOOTER1
_NIMBUS_FOOTER2
_NIMBUS_FOOTER3
