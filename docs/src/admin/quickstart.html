m4_include(/mcs/m4/worksp.lib.m4)
_NIMBUS_HEADER(2.4 Admin Installation)
_NIMBUS_HEADER2(n,n,y,n,n,n,n)
_NIMBUS_LEFT2_COLUMN
_NIMBUS_LEFT2_ADMIN_SIDEBAR(n,y,n,n,n)
_NIMBUS_LEFT2_COLUMN_END
_NIMBUS_CENTER2_COLUMN
_NIMBUS_2_4_DEPRECATED

<h2>Nimbus 2.4 Admin Installation</h2>

<p>
    The document assumes the services node you are working on is Linux or
    some UNIX variant (such as OSX perhaps).  The VMM nodes are required to
    be Linux.  Most testing is carried out on Linux currently.
</p>

<p>
    There are two system accounts involved:
</p>
<ul>
    <li>
        <p>
            <b>privileged account</b> - Pick a privileged account to run the
            remote client facing service.  A privileged account on the VMM nodes
            is necessary as well.
            This guide assumes the privileged account on the service node and
            VMM nodes have the same name but that is not strictly necessary.
        </p>
        <p>
            In this guide (especially in the command samples) we will refer
            to an account of this type named <i>nimbus</i> with terminal
            prompts like "<b>nimbus $</b>"
        </p>
		<p>
			Note: this is not a privileged account in the sense that root is a
			privileged account.  This is a regular account that you will give
			special powers to during the setup process.  It is an account that
			will be privileged because it will have control over the VMMs.
		</p>
    </li>
    <li>
        <p>
            <b>superuser account</b> - The root account is necessary to install
            dependencies on the VMM nodes (Xen/KVM, ebtables, etc.) and also to
            install the Nimbus agent that lives on the VMM nodes (workspace
            control).
        </p>
        <p>
            In this guide (especially in the command samples) we will refer
            to an account of this type named <i>root</i> with terminal
            prompts like "<b>root #</b>"
        </p>
    </li>
</ul>

<p>
    The installation guide is broken up into these steps:
</p>

<ul style="list-style: disc">
    <li>
        <p>
            <a href="#part-I">Part I</a> - Install/verify dependencies
        </p>
        <ul style="list-style: disc">
            <li>I.A. <a href="#part-Ia">Service node</a></li>
            <li>I.B. <a href="#part-Ib">Xen/KVM and libvirt</a></li>
            <li>I.C. <a href="#part-Ic">Misc. libraries for the VMM nodes</a></li>
        </ul>
    </li>
    <li>
        <p>
            <a href="#part-II">Part II</a> -
            Install/verify the Nimbus service package
        </p>
        <ul style="list-style: disc">
            <li>II.A. <a href="#part-IIa">Download and install</a></li>
            <li>II.B. <a href="#part-IIb">Necessary configurations</a></li>
            <li>II.C. <a href="#part-IIc">Test call</a></li>
        </ul>
    </li>
    <li>
        <p>
            <a href="#part-III">Part III</a> - Install/verify workspace-control
        </p>
        <ul style="list-style: disc">
            <li>III.A. <a href="#part-IIIa">Download and install</a></li>
            <li>III.B. <a href="#part-IIIb">Necessary configurations</a></li>
            <li>III.C. <a href="#part-IIIc">Testing</a></li>
        </ul>
    </li>
    <li>
        <p>
            <a href="#part-IV">Part IV</a> - End to end test
        </p>
    </li>
</ul>

<br />





<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->





<a name="part-I"> </a>
<h2>Part I: Install/verify dependencies</h2>

<ul>
    <li style="list-style: disc">I.A. <a href="#part-Ia">Service node</a></li>
    <li>I.B. <a href="#part-Ib">Xen/KVM and libvirt</a></li>
    <li>I.C. <a href="#part-Ic">Misc. libraries for the VMM nodes</a></li>
</ul>

<a name="service-node"> </a>
<a name="part-Ia"> </a>
<h3>I.A. Service node _NAMELINK(part-Ia) _NAMELINK(service-node)</h3>
<p>
    The Nimbus service node installation is now largely self-contained. Starting
    with version 2.4, Nimbus no longer needs to be deployed into an existing Globus
    container. The container is automatically embedded into the Nimbus installation.
</p>

<p>
    The service node installer has the following dependencies:
</p>
<ul>
    <li>Sun Java 1.5 or later. The java command must be on your path</li>
    <li>Apache Ant</li>
    <li>Python 2.4 or later</li>
    <li>If you plan to use the Nimbus web application (disabled by default),
        you must also have these Python modules installed:
        pyOpenSSL and python-sqlite2.
    </li>
</ul>

<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->



<a name="part-Ib"> </a>
<h3>I.B. Xen/KVM and libvirt _NAMELINK(part-Ib)</h3>
<p>
    The workspace service manages <a href="http://www.xen.org">Xen</a>
    or <a href="http://www.linux-kvm.org">KVM</a> VMs.  It interfaces with both
    of these systems using a library called 
    <a href="http://libvirt.org/">libvirt</a>.
</p>
<p>
    The workspace service itself does not need to run on a VMM node.  You
    must pick VMM nodes to install Xen or KVM on, the workspace service will
    manage these.  Initially, pick just one node to install and test
    with.
    When all goes well, add more into the pool of VMMs the workspace service
    can send work to.
</p>
<p>
    You need to install Xen/KVM and get a basic VM running with bridged
    networking.
</p>
<p>
    Nimbus has been most heavily tested with the Xen 3.1 series although it
    is known to be working with 3.2 (this may require the "tap:aio"
    configuration
    in workspace-control's "xen.conf" configuration file).  There is no
    information yet about Nimbus on the new/upcoming Xen 3.3 series.  Nimbus
    has also been tested with QEMU/KVM 0.11.0
</p>

<p>
	The VMM nodes require Python 2.4 and libvirt 0.6.3.  Python 2.3 should work but it
	untested and unsupported.
</p>

<p>
    You can actually put off the whole VMM step until later if you just want to evaluate
    the Nimbus clients and services:  right out of the box Nimbus makes "fake"
    calls to workspace control (which is the agent that actually manages things
    on the VMM nodes).  This allows the service side to be tested independently
    of the VMM.  You "join them up" by turning off the "fake" switch when you're
    ready to do end to end testing.
</p>

<a name="acquire-install-vmm"> </a>
<h4>* Acquire and install VMM software: _NAMELINK(acquire-install-xen)</h4>

<p>
    Your Linux distribution probably has good support for libvirt, Xen and KVM
    (Debian/Ubuntu, RedHat, Gentoo, SUSE, etc.) in which case it is best to
    defer to distribution specific notes about how to get Xen or KVM up and
    running (with both libvirt and bridged networking).
</p>
<p>
    If your Linux distribution doesn't support these packages well, you can always try installing from source or binary, starting at <a href="http://libvirt.org/">libvirt.org</a> (and either <a href="http://xen.org/">xen.org</a> or <a href="http://www.linux-kvm.org">linux-kvm.org</a>).
</p>

<p>
    <b>Note:</b> an important step is to make sure that your libvirt 
    installation supports the libvirt "python bindings."  This is how the
    Nimbus software actually interacts with libvirt.  In some cases, the
    bindings (which come with a normal libvirt installation) are not installed.
    On some systems you need to trigger this by installing the "python-dev"
    package first.  In order to check you have the right install, Nimbus
    includes a script in the workspace-control download.
</p>

_EXAMPLE_ROOTCMD_BEGIN
./sbin/test-dependencies.sh
_EXAMPLE_CMD_END

<p>
    If this script reports you have a problem, look into your distribution's
    libvirt support and don't hesitate to contact the
    <a href="_NIMBUS_WEBSITE/contact/">workspace-user</a> mailing list with
    problems.
</p>

<a name="test-xen"> </a>
<h4>* Test Xen: _NAMELINK(test-xen)</h4>

<p>
    You can test Xen with any Xen VM image, mostly.  In this guide we will use a
    small and simple VM image as an example.  Look
    <a href="_SCIENCECLOUDS_WEBSITE/marketplace/#other">here</a> for some
    ideas about where to find Xen VM images.  You can also create them from
    scratch of course, if you have the time.
</p>

<p>
    In particular, we will be using a ttylinux-based image. ttylinux is a very
    small, yet functional, Linux distribution requiring only around 4 MB.
    Visit the <a href="http://www.minimalinux.org/ttylinux/">ttylinux</a>
    home page for a list of some of its many nice features.
</p>

<p>
    You can download a tarball containing the image
    <a href="_NIMBUS_WEBSITE/downloads/ttylinux-xen-libvirt.tar.gz">here</a>.
    This directory should contain the following files:
</p>
<ul>
    <li>
        ttylinux-xen.img: The partition image
    </li>
    <li>
        libvirt-ttylinux.xml: A sample libvirt configuration
    </li>
</ul>

<p>
    Take into account that the provided ttylinux image is not the exact same
    image you can download from the ttylinux home page. It is preconfigured
    to obtain a network address through DHCP, and depends on a different root
    device than a regular ttylinux image (to make the image Xen-friendlier).
</p>

<p>
    Test the image using the provided configuration file. First of all, make
    sure you replace the values of the kernel and disk parameters inside the
    file with appropriate values. In particular, you should point kernel to
    your Xen guest kernel and disk to the location of the ttylinux-xen file.
</p>

<p>
    To test the image, run the following <b>as root for now</b> (see below):
</p>

_EXAMPLE_ROOTCMD_BEGIN
virsh -c "xen:///" create libvirt-ttylinux.xml
_EXAMPLE_CMD_END

<p>
    Logging in via terminal using:
</p>

_EXAMPLE_ROOTCMD_BEGIN
vncviewer localhost::5900
_EXAMPLE_CMD_END

<p>
    You should see ttylinux boot messages, followed by a login prompt. You can
    log in using user 'root' and password 'root'. You can use ifconfig to check
    that the networking interface (eth0) is correctly setup. If you do not have
    a DHCP server, you can also use ifconfig to configure eth0 with a static
    IP address. Once the network is correctly set up, you should be able to
    ping out to some other machine besides dom0 and from that other machine
    be able to ping this address. Note that you should do all this just for
    the purposes of verifying that the image works correctly.
</p>
<p>
    Keep the image configured to obtain a network address automatically via
    DHCP (even if your network doesn't have a DHCP server), as the Workspace
    Service will use DHCP to dynamically set up networking in it.
</p>
<p>
    <b>Note</b>: most Xen documentation you will find online about
    testing and troubleshooting the install
    is <i>valid</i> in this situation because you are not doing anything
    specifically related to Nimbus here.
</p>
<p>
    <b>Note</b>: make sure you read the section below about <a href="#libvirt-connection">libvirt connection strings</a>.
</p>

<p>
    <b>Problem</b>: the way libvirt uses Xen, it is not possible to quickly set up a dedicated account that has administrator access to the hypervisor.  If you adjust the permissions on the unix domain socket Xen can be controlled with, it does not result in the right setup.  To work around this issue there seem to be two OK options:
</p>
<ol>
    <li>
        <p>Set up the Xend daemon to accept TCP traffic but limit to localhost traffic from a specific account.  This can be accomplished using iptables and the uid-owner rule (limit INPUT traffic to the port from localhost, limit any local OUTPUT to that localhost:port to specific users).</p>
    </li>
    <li>
        <p>Make workspace-control a wrapper program that inserts sudo arguments before invoking the real workspace-control.</p>
    </li>
</ol>

<a name="test-kvm"> </a>
<h4>* Test KVM: _NAMELINK(test-kvm)</h4>

<p>
    To test KVM we do not have a ready made image for you yet.  Use an image
    you have already gotten to run using "virsh -c qemu:///system"
</p>

<p>
    <b>Note</b>: make sure you read the section below about <a href="#libvirt-connection">libvirt connection strings</a>.
</p>

<a name="libvirt-connection"> </a>
<h4>* Libvirt connection: _NAMELINK(libvirt-connection)</h4>

<p>
    The workspace-control tool will be operating from a privileged UNIX account
    that is NOT root.  The best way to allow this account to interface with
    libvirt is to use UNIX domain sockets and group permissions.  See
    libvirtd.conf (usually located in the /etc directory) for details.
</p>
<p>
    The connection strings for KVM and Xen
    will usually be similar to "qemu:///system" and
    "///var/run/xend/xend-socket" respectively for this method.
</p>
<p>
    Ultimately do what is best for your deployment situation, the details can be
    found on the <a href="http://libvirt.org/uri.html">libvirt site</a>.
    Whatever connection URI you choose to use, it must be configured in
    workspace-control's "libvirt.conf" file.
</p>
<p>
    If you are using Xen, see the special note above.
</p>

<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->


<a name="part-Ic"> </a>
<h3>I.C. Misc. libraries for the VMM nodes _NAMELINK(part-Ic)</h3>
<p>
    There are a few libraries needed on the VMM nodes before workspace control
    will work.
</p>

<p>
    The <a href="http://www.isc.org/index.pl?/sw/dhcp/" target="_top">ISC
    DHCP server</a> (or DHCP server with compatible conf file)
    and <a href="http://ebtables.sourceforge.net/" target="_top">ebtables</a>
    are required to be running on each hypervisor node.
</p>

<p>
    Any recent version of each package should be compatible, the scripts
    distributed with workspace control that automate the configurations were
    tested with ISC DHCP 3.0.3 and ebtables 2.0.6 userspace tools.
</p>

<p>
    For more information on why this software is now necessary and
    how it will not interfere with a site's pre-existing DHCP server, see
    <a href="reference.html#backend-config-invm-networking">the
    network configuration details section</a> in the reference guide.
</p>

<p>
    Since these two pieces of software are relatively common, they may
    already be present on your hypervisor nodes via the package management
    system.  Check your distribution tools  for packages called
    <i>dhcp</i> (ISC DHCP server) and <i>ebtables</i>. You can also check
    for the existence of <i>/sbin/ebtables</i> or <i>/usr/sbin/ebtables</i>
    (for ebtables) and any of the following files for the DHCP server:
</p>

<ul>
    <li>
        <p>
            <i>/etc/dhcp/dhcpd.conf</i>
        </p>
    </li>

    <li>
        <p>
            <i>/etc/dhcp3/dhcpd.conf</i>
        </p>
    </li>

    <li>
        <p>
            <i>/etc/init.d/dhcpd</i>
        </p>
    </li>

    <li>
        <p>
            <i>/etc/init.d/dhcp3-server</i>
        </p>
    </li>
</ul>

<p>
    If these software packages are not installed, all major Linux distributions
    include them and you should be able to easily install them with your package
    management system.  For example, "rpm -ihv dhcp-*.rpm", "apt-get install
    dhcp", "emerge dhcp", etc.  And similarly for   ebtables.
</p>

<p>
    ebtables requires kernel support in dom0, the default Xen kernel
    includes this support.  If your dom0 kernel does not include these
    for some reason, the options to enable are under Networking ::
    Networking options :: Network packet filtering :: Bridge Netfilter
    Configuration :: Ethernet Bridge Tables
</p>

<p>
    workspace-control also requires
    <a href="http://www.courtesan.com/sudo/">sudo</a> and Python 2.3+ on all
    VMM nodes under the control of the Workspace Service.
</p>



<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->




<a name="part-Id"> </a>
<h3>I.D. SSH _NAMELINK(part-Id)</h3>
<p>
    Finally, the privileged account on the service node needs to be able to
    SSH freely to the VMM nodes (without needing a password).  And vice versa,
    the privileged account on the VMM nodes needs to be able to freely SSH
    back to the service nodes to deliver notifications.
</p>

<p>
    You will be given the opportunity to get this right later during the service
    auto-configuration steps.  The relevant security setups will be tested
    interactively, making sure you get this right.
</p>


<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->


<br />
        
<a name="part-II"> </a>
<h2>Part II: Install/verify the Nimbus service package _NAMELINK(part-II)</h2>

<ul style="list-style: disc">
    <li>II.A. <a href="#part-IIa">Download and install</a></li>
    <li>II.B. <a href="#part-IIb">Necessary configurations</a></li>
    <li>II.C. <a href="#part-IIc">Test call</a></li>
</ul>



<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->



<a name="part-IIa"> </a>
<h3>II.A. Download and install _NAMELINK(part-IIa)</h3>

<h4>* Retrieve and unpack:</h4>
<p>
    Grab the main Nimbus archive from the <a href="_NIMBUS_WEBSITE/downloads">downloads</a>
    page and unpack the archive:
</p>

_EXAMPLE_CMD_BEGIN
wget http://www.nimbusproject.org/downloads/nimbus-2.4.tar.gz
_EXAMPLE_CMD_END

_EXAMPLE_CMD_BEGIN
tar xzf nimbus-2.4.tar.gz
_EXAMPLE_CMD_END


<a name="build-install"> </a>
<h4>* Build and install: _NAMELINK(build-install)</h4>
<p>
    Starting with Nimbus version 2.4, you do not need to run the Ant install
    scripts directly. You can run a single command to set up the Nimbus
    installation environment, build and deploy the software, and perform
    basic configuration. First change into the unpacked archive directory.
</p>

_EXAMPLE_CMD_BEGIN
cd nimbus-2.4
_EXAMPLE_CMD_END

<p>
    Then run the install program, specifying a destination path where you
    would like Nimbus to be installed. If this path exists, it must be an
    empty directory into which you can read and write. If the path does not
    exist, you must have write access to the parent directory (so the installer
    can create a new directory).
</p>

_EXAMPLE_CMD_BEGIN
./bin/install /destination/path
_EXAMPLE_CMD_END

<p>
    This command will initialize the Nimbus home directory at the specified path.
    It will install a service container under the "services/" directory. It will
    then build and install Nimbus from source and deploy it to the container.
    Finally, it will run the "bin/nimbus-configure" program to help you set up
    an operational install. Follow the instructions provided by this program.
</p>

<p>
    Hereafter in this guide, the Nimbus destination path you specified will be
    referred to as <i>$NIMBUS_HOME</i>.
</p>

<p>
    Several Nimbus components are built and installed by default:
</p>

<ul>
    <li>
        The Java based RM API and workspace service - VM/VMM manager
    </li>
    <li>
        Default clients
    </li>
    <li>
        WSRF frontend - a remote protocol implementation
        compatible with the default clients
    </li>
    <li>
        EC2 SOAP frontend - a remote protocol implementation
        compatible with EC2 SOAP clients
    </li>
    <li>
        EC2 Query frontend - a remote protocol implementation
        compatible with EC2 Query clients (such as boto or typica).
        This frontend is separate from the standard service
        container. It listens on its own port.
    </li>
    <li>
        Nimbus Context Broker - a service which is used to assemble
        virtual clusters on-the-fly.
    </li>
</ul>

<p>
    For more information about what these things are, see the
    <a href="../faq.html">FAQ</a>.
</p>
<p>
    You will not be able to run against the EC2 frontends until you have
    set up the cloud configuration because it relies on users having a
    personal repository directory (out of scope of this document).
</p>

<p>
    After the install program finishes, you should have a nearly complete
    Nimbus installation in the $NIMBUS_HOME directory. Take a look, it
    contains several important files and directories.
</p>
<ul>
    <li>
        <b>bin/</b> - contains programs for managing and configuring Nimbus.
        "bin/nimbus-configure" was run at the end of the install and can be
        rerun to adjust the same configuration options. "bin/nimbusctl" is used
        start and stop Nimbus services.
    </li>
    <li>
        <b>services/</b> - GT container into which Nimbus has been deployed.
        Most of the configuration files live under here, in "etc/nimbus/".
    </li>
    <li>
        <b>var/</b> - has a simple Certificate Authority which is used by the
        Context Broker and which, in the default install, can also be used for
        host and user certificates. This directory also contains log files for
        the running service.
    </li>
    <li>
        <b>web/</b> - the Nimbus web application. By default this is not enabled.
    </li>
    <li>
        <b>nimbus-setup.conf</b> - This file is written by the "bin/nimbus-configure"
        program. It contains configuration values. If you wish to change one of these
        values, you can usually edit this file and rerun "bin/nimbus-configure".
    </li>
</ul>

<p>
    At this point in the installation process, Nimbus is installed and configured
    in "fake" mode, in which the services run and accept requests, but no actual
    VMM nodes are involved. The following network ports are used:
</p>
<ul>
    <li>
        <b>Service container - port 8443</b> - edit this in
        "$NIMBUS_HOME/sbin/run-services.sh"
    </li>
    <li>
        <b>EC2 Query frontend - port 8444</b> - edit this in
        "$NIMBUS_HOME/services/etc/nimbus/query/query.conf"
    </li>
</ul>



<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->



<a name="part-IIb"> </a>
<h3>II.B. Necessary configurations _NAMELINK(part-IIb)</h3>

<p>
    There are a few configurations that cannot have any defaults.
</p>

<p>
    We provide an <b>auto-configuration</b> program
    to gently take you through these configurations.  During the process,
    several tests will be made to verify your set up.
</p>

<p>
    This is installed by default, run the following command to get started:
</p>

_EXAMPLE_CMD_BEGIN
$NIMBUS_HOME/services/share/nimbus-autoconfig/autoconfig.sh
_EXAMPLE_CMD_END

<p>
    Otherwise you can refer to
    <a href="reference.html#manual-nimbus-basics">this section</a>
    of the reference guide to see the old instructions.
    Those instructions may shed some light on certain
    configurations as you move past the testing stage and want to know
    more about what is happening (but honestly, the best place to look
    for configuration insight is in the .conf file inline comments).
</p>


<a name="authorization"> </a>
<h4>* Authorization: _NAMELINK(authorization)</h4>
<p>
    In the default install, Nimbus uses two files to manage authorization
    of users on the remote interfaces: users with X.509 credentials are
    checked against a grid-mapfile and users of the EC2 Query frontend
    are checked against the query users.txt list (and mapped to an
    X.509 Distinguished Name).
</p>

<p>
    The grid-mapfile is located at "$NIMBUS_HOME/services/etc/nimbus/nimbus-grid-mapfile"
    by default, but you can have Nimbus use another file by editing the "gridmap" entry
    of "$NIMBUS_HOME/nimbus-setup.conf" and rerunning the nimbus-configure program.
</p>
<p>
    A grid-mapfile is basically an access control list.  It says which remote
    identities can access the container or a specific service. Note however that
    you need to ensure that any identity specified in this file is from a "trusted"
    Certificate Authority. CAs are trusted by placing their certificate in the
    trusted-certs directory (by default: "$NIMBUS_HOME/var/ca/trusted-certs/").
</p>

<p>
    Users of the EC2 Query frontend do not provide X.509 credentials, but instead
    use symmetric authentication tokens. These tokens are matched against entries
    in the query users file. By default, this file is located at
    "$NIMBUS_HOME/services/etc/nimbus/query/users.txt".
</p>

<p>
    In the cloud configuration you will see that there is a handy way to make the
    grid-mapfile (and the users.txt file) just a basic
    entry barrier.  The real authorization decision can be made by more fine
    grained policies on a per user basis (for example, limiting certain users to
    certain amounts of total VM time, etc.).
</p>


<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->



<a name="part-IIc"> </a>
<h3>II.C. Test call _NAMELINK(part-IIc)</h3>

<p>
    If you used the auto-configuration program, you are
    not ready for a live test yet.  You will come back to this section
    after configuring workspace-control.  <a href="#part-III">Jump there now</a>.
</p>
<p>
    If you did <i>not</i> follow the auto-configuration program, currently Nimbus is
    set to "fake" mode.  This allows you to get the service and VMM nodes
    working independently before you "join them up" for the live end to end
    test.
</p>

<a name="user-cert"> </a>
<h4>* Set up user certificate _NAMELINK(user-cert)</h4>

<p>
    <b>Note:</b> There is a known bug for Python 2.4- users that is preventing them from executing this step correctly. If you have
a Python version lower than 2.5, please substitute the $NIMBUS_HOME/sbin/nimbus-new-cert.py for the fixed script available <a href="http://github.com/nimbusproject/nimbus/raw/99dee6e10e63b8b4c7b29fe398bf1807d5947ddb/home/sbin/nimbus-new-cert.py">here</a>. We apologize about this issue. This fix is going to be present in the next release.
</p>

<p>
    Before starting, you should create a X.509 user credential to access the
    workspace service if you haven't done that yet. We provide a tool to create
    the user certificate and key from the embedded Nimbus Certificate Authority.
    Run the following command to start the tool, and follow its instructions:
</p>

_EXAMPLE_CMD_BEGIN
$NIMBUS_HOME/bin/nimbus-new-cert
_EXAMPLE_CMD_END

<p>
    If everything went well, the new certificate and key should be created in
    $HOME/.globus, and the DN of the certificate should be displayed in the program output:
</p>

<div class="screen">
Success! The DN of the new certificate is:

    "/O=Auto/OU=NimbusCA/CN=Bob"
</div>

<p>
    In order to enable access to the workspace Service for some credential,
    its DN should be added to the nimbus-grid-mapfile, located at 
    "$NIMBUS_HOME/services/etc/nimbus/nimbus-grid-mapfile".
    Add a line like this, using the DN from the generated certificate:
</p>
<div class="screen">
"/O=Auto/OU=NimbusCA/CN=Bob" test_account
</div>

<a name="start-nimbus"> </a>
<h4>* Start Nimbus services _NAMELINK(start-nimbus)</h4>

<p>
    You can now start the Nimbus services using the '$NIMBUS_HOME/bin/nimbusctl" command:
</p>

_EXAMPLE_CMD_BEGIN
nimbusctl start
_EXAMPLE_CMD_END

<p>
    It may take several seconds for everything to initialize. You should check the output
    in the logfile to ensure that the services started up okay:
</p>

_EXAMPLE_CMD_BEGIN
tail -f $NIMBUS_HOME/var/services.log
_EXAMPLE_CMD_END


<a name="test-client"> </a>
<h4>* Client: _NAMELINK(test-client)</h4>

<p>
    Set up needed environment variables by sourcing the
    generated 'environment.sh' file, then run this program:
</p>

_EXAMPLE_CMD_BEGIN
. $NIMBUS_HOME/sbin/environment.sh
_EXAMPLE_CMD_END

_EXAMPLE_CMD_BEGIN
cd $GLOBUS_LOCATION
_EXAMPLE_CMD_END

_EXAMPLE_CMD_BEGIN
./bin/workspace
_EXAMPLE_CMD_END


<div class="screen"><pre>Problem: You must supply an action.
See help (-h).</pre>
</div>

<p>
    OK, let's check out help, then.
</p>

_EXAMPLE_CMD_BEGIN
./bin/workspace -h
_EXAMPLE_CMD_END

<p>
    See sample output <a href="workspace-client-help.txt">here</a>.
</p>

<p>
    A lot of options.  This is the scriptable reference client.  The cloud
    client is more user friendly.  The cloud configuration which supports
    the cloud client is the recommended setup to provide users access to.
    This is very much because the cloud client offers a low entry barrier to
    people that just want to start getting work done.
</p>

<p>
    For this guide we are going to run two of the actions listed in help,
    <i>--deploy</i> and <i>--destroy</i>.
    You can experiment with other ones, each action has its own help section.
</p>

_NAMELINK(deploy-test)
<h4>* Deploy test: _NAMELINK(deploy-test)</h4>

<p>
    Grab this <a href="test-create.sh">test script</a>.
</p>

_EXAMPLE_CMD_BEGIN
wget http://www.nimbusproject.org/docs/2.4/admin/test-create.sh
_EXAMPLE_CMD_END

<p>
    This script references a sample deployment file located at:
    "$NIMBUS_HOME/services/share/nimbus-clients/sample-workspace.xml".
    You may need to edit this file to specify an image that exists
    in your VMM's "/opt/nimbus/var/workspace-control/images/" directory.
</p>

<p>
    Run it:
</p>

_EXAMPLE_CMD_BEGIN
sh test-create.sh
_EXAMPLE_CMD_END

<p>
    Sample successful output is <a href="test-create-output.txt">here</a>
</p>

<p>
    As you can see, an IP address was allocated to the VM, a schedule given,
    and then some state changes reported.
</p>
<p>
    If you opened a third terminal to destroy, you could see the state change
    move after the destruction, too.  Or you could type <i>CTRL-C</i> to exit
    this command and run destroy in the same terminal.
</p>

_EXAMPLE_CMD_BEGIN
./bin/workspace -e test.epr --destroy
_EXAMPLE_CMD_END     

<p>
    ... and we get something like:
</p>

<div class="screen"><pre>Destroying workspace 2 @ "https://10.20.0.1:8443/wsrf/services/WorkspaceService"... destroyed.</pre>
</div>

<p>
    If you look at this file, you will now see some usage recorded:
</p>

_EXAMPLE_CMD_BEGIN
cat $NIMBUS_HOME/services/var/nimbus/accounting-events.txt
_EXAMPLE_CMD_END


<p>
    The "CREATED" line is a record of the deployment launch.  A reservation
    for time is made.
</p>

<p>
    The "REMOVED" line is a record of the destruction.  A recording of the
    actual time used is made.  These actual usage recordings stay long term
    in an internal accounting database and (along with any current reservations)
    can be used to make authorization decisions on a per-user basis.
</p>



<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->




<br />

<a name="part-III"> </a>
<h2>Part III: Install/verify workspace-control _NAMELINK(part-III)</h2>

<ul style="list-style: disc">
    <li>III.A. <a href="#part-IIIa">Download and install</a></li>
    <li>III.B. <a href="#part-IIIb">Necessary configurations</a></li>
    <li>III.C. <a href="#part-IIIc">Testing</a></li>
</ul>


<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->

<a name="part-IIIa"> </a>
<h3>III.A. Download and install _NAMELINK(part-IIIa)</h3>

<p>
    Download the "Control Agents" tar file from the download page, and untar it.
    This archive contains both workspace-control and the workspace pilot. For this
    configuration we are using workspace-control, so copy it to the destination
    directory.
</p>

<p>
    This guide assumes you are using "/opt/nimbus" as the target directory
    of the install.  You need root privileges to complete the installation.
</p>

<a name="create-priv-user-and-group"> </a>
<h4>* Create privileged user: _NAMELINK(create-priv-user)</h4>
        
<p>
    First, you need to choose (or create) a user that will be
    used to run the backend script (for this guide, let's assume that your
    user is called nimbus). Since we do not allow the backend
    script to be run as root, this user will rely on sudo to run Xen commands
    and other privileged commands.
</p>

<p>
    Next, as root you should change permissions like so:
</p>

_EXAMPLE_ROOTCMD_BEGIN
cd /opt/nimbus
_EXAMPLE_CMD_END

_EXAMPLE_ROOTCMD_BEGIN
chown -R root bin etc lib libexec src
_EXAMPLE_CMD_END

_EXAMPLE_ROOTCMD_BEGIN
chown -R nimbus var
_EXAMPLE_CMD_END

_EXAMPLE_ROOTCMD_BEGIN
find . -type d -exec chmod 775 {} \;
_EXAMPLE_CMD_END

_EXAMPLE_ROOTCMD_BEGIN
find . -type f -exec chmod 664 {} \;
_EXAMPLE_CMD_END

_EXAMPLE_ROOTCMD_BEGIN
find bin sbin libexec -iname "*sh" -exec chmod 755 {} \;
_EXAMPLE_CMD_END

<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->

<a name="part-IIIb"> </a>
<h3>III.B. Necessary configurations _NAMELINK(part-IIIb)</h3>


<h4>* Configure sudo:</h4>

<p>
    Using the visudo command, add the sudo policies printed out by the
    installer to the /etc/sudoers file. These policies should look
    something like this.
</p>

<div class="screen"><pre>
nimbus ALL=(root) NOPASSWD: /opt/nimbus/libexec/workspace-control/mount-alter.sh
nimbus ALL=(root) NOPASSWD: /opt/nimbus/libexec/workspace-control/dhcp-config.sh
nimbus ALL=(root) NOPASSWD: /opt/nimbus/libexec/workspace-control/xen-ebtables-config.sh
</pre>
</div>

<p>
    These policies reflect the user that will be running workspace control
    (nimbus) and
    the correct full paths to the libexec tools.  See 
    "/opt/nimbus/etc/workspace-control/sudo.conf" for more information
</p>

<p>
    Also note that there is a specific ebtables script for xen and kvm.
</p>

<p>
    The Xen ebtables script is configured by default.
    If you are using KVM, you must configure the "kvm-ebtables-config.sh"
    script in two places.  First in the sudo rules so that it can be invoked
    (see workspace-control's "sudo.conf" file for details). Second, in
    workspace-control's "networks.conf" file.
</p>

<p>
    <b>Note:</b> currently the KVM ebtables script can only support spoofing
    protection when there is one KVM virtual machine running at a time on
    each VMM node (this is the most common deployment configuration for sites 
    supporting science).  Nimbus' Xen support allows many guest VMs to be
    running while also ensuring there is no MAC and IP address spoofing.
</p>

<p>
    You may need to comment out any "requiretty" setting in the sudoers policy:
</p>

<div class="screen"><pre>
#Defaults    requiretty
</pre>
</div>

<p>
    The commands run via sudo are not using a terminal and so if you have
    "requiretty" enabled, this can cause a failure.
</p>

<a name="dhcp-conf"> </a>
<h4>* Configure DHCP: _NAMELINK(dhcp-conf)</h4>

<p>
    DHCP is used here as a delivery mechanism only, these DHCP servers do NOT
    pick the addresses to use on their own. Their policy files are dynamically
    altered by workspace-control as needed. Policy additions include the MAC
    addresses which is used to make sure the requester receives the intended
    DHCP lease.
</p>

<p>
    Configuring the DHCP server consists of copying the example DHCP file
    "dhcp.conf.example" (included in the "share/workspace-control" directory) to
    "/etc/dhcp/dhcpd.conf" and editing it to include the proper subnet lines
    (see the contents of the example file). The subnet lines are necessary
    to get the DHCP server to listen on the node's network interface. So,
    make sure that you add a subnet line that matches the subnet of the
    node's network interface. No lease configurations, available ranges, etc.
    should be added: these are added dynamically to the file after the token
    at the bottom.
</p>

<p>
    In most cases it is unecessary, but if you have a non-standard DHCP
    configuration you may need to look at the "dhcp-config.sh" script in the
    protected workspace bin directory and look at the "adjust as necessary"
    section. The assumptions made are as follows:
</p>

<ul>
    <li>DHCP policy file to adjust: "/etc/dhcp/dhcpd.conf"</li>
    <li>Stop DHCP server: "/etc/init.d/dhcpd stop"</li>
    <li>Start DHCP server: "/etc/init.d/dhcpd start"</li>
    <li>The standard unix utility "dirname" is assumed to be
        installed. This is used to find the workspace-control
        utilities "dhcp-conf-alter.py" and "ebtables-config.sh",
        we assume they are in the same directory as "dhcp-config.sh"
        itself. Paths to these can alternatively be hardcoded to fit
        your preferred configuration.</li>
</ul>

<p>
    The "aux/foreign-subnet" script (in the workspace control
    source directory) may be needed for DHCP support. It allows
    VMMs to deliver IP information over DHCP to workspaces even if the VMM
    itself does not have a presence on the target IP's subnet. This is an
    advanced configuration, you should read through the script's leading
    comments and make sure to clear up any questions before using. It is
    particularly useful for hosting workspaces with public IPs where the VMMs
    themselves do not have public IPs. This is because it does not require
    a unique interface alias for each VMM (public IPs are often scarce
    resources).
</p>


<a name="kernel-conf"> </a>
<h4>* Configure kernel(s): _NAMELINK(kernel-conf)</h4>

<p>
    Copy any kernels you wish to use to the <i>/opt/nimbus/var/workspace-control/kernels</i>
    directory, and list them in the <i>authz_kernels</i> option in the <i>kernels.conf</i> configuration file.
    By doing this, clients can choose from these kernels in the metadata,
    but they must already exist at the hypervisor node and must be in the
    guestkernels list.
</p>

<a name="net-conf"> </a>
<h4>* Configure network(s): _NAMELINK(net-conf)</h4>

<p>
    In the workspace-control <i>networks.conf</i> configuration
    file, find notes about specifying the bridge name to use.
</p>



<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->

<a name="part-IIIc"> </a>
<h3>III.C. Testing _NAMELINK(part-IIIc)</h3>

<p>
    For testing with a real VM (see testing section below) using that
    <a href="test-create.sh">test-create.sh</a> script, add your test VM
    from the <a href="#part-Ib">Xen</a> section.
</p>

<p>
    The script is expecting a file named "ttylinux-xen" in the
    <i>/opt/nimbus/var/workspace-control/images</i> directory because of the
    <i>file://ttylinux-xen</i> line in the xml definition file the
    script points to
    (<i>$NIMBUS_HOME/services/share/nimbus-clients/sample-workspace.xml</i>).
</p>




<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->



<br />

<a name="part-IV"> </a>
<h2>Part IV: End to end test _NAMELINK(part-IV)</h2>

<p>
    Now revisit the service <a href="#part-IIc">Test call</a> section.
</p>

<p>
    If you did <i>not</i> use the <a href="#part-IIb">auto-configuration</a> program:
</p>

<p>
   You are now ready to turn "fake" mode off and try a real VM launch.  Back
   on the Nimbus services node:
</p>

_EXAMPLE_CMD_BEGIN
nano -w $NIMBUS_HOME/services/etc/nimbus/workspace-service/other/common.conf
_EXAMPLE_CMD_END

<p>
    ... and change the "fake.mode" setting to "false"
</p>


<div class="screen"><pre>
fake.mode=false
</pre></div>

<p>
    Now revisit the service <a href="#part-IIc">Test call</a> section.
</p>


<!--- ********************************************************************** -->
<!--- ********************************************************************** -->
<!--- ********************************************************************** -->


<br />
<br />

<hr />
<hr />

<p>
    Do not hesitate to contact the
    <a href="_NIMBUS_WEBSITE/contact/">workspace-user</a> mailing list with
    problems.
</p>
<p>
    We plan to streamline some of the steps and also significantly add to the
    troubleshooting and reference sections.
</p>

<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />

_NIMBUS_CENTER2_COLUMN_END
_NIMBUS_FOOTER1
_NIMBUS_FOOTER2
_NIMBUS_FOOTER3
